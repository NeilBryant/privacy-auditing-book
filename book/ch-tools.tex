\chapter{Open Source Forensic Tools}

\section{Sleuth Kit}

Sleuth Kit is an open source digital forensics toolkit for extracting
files from disk images. Sleuth Kit understands a variety of disk image
formats, partitioning schemes, and file systems. With Sleuth Kit, you can can recover
both allocated files and files that have been deleted directly from a
disk image, without having to ``mount'' the disk image by the host
operating system. An added advantage of Sleuth Kit is that it runs on
Windows, Linux, and Macintosh systems, allowing you to access data
from disk images even if your underlying operating system does not
understand the format.

The Sleuth Kit website\footnote{http://sleuthkit.org} has information
on Sleuth Kit, Autopsy (a graphical interface for SleuthKit), the
Sleuth Kit Hadoop Framework (for processing large numbers of hard
drives in a cloud computing environment), and other tools. 
Precompiled binaries for

\subsection{Sleuth Kit under Windows}
Windows users should download pre-compiled Sleuth Kit binaries from Source
Forge.\footnote{\url{http://sourceforge.net/projects/sleuthkit/files/sleuthkit/4.0.2/}}
and install them in into |c:\sleuthkit|.

\subsection{Sleuth Kit under Linux}

\subsection{Sleuth Kit under Macintosh}

\section{Network Monitoring with WireShark and tcpflow}

\subsection{WireShark and tcpflow on Windows}
\subsection{WireShark and tcpflow on Linux}
\subsection{WireShark and tcpflow on Macintosh}

\section{Bulk Data Analysis with Bulk Extractor}

\subsubsection{Bulk Extractor on Windows}
\subsubsection{Bulk Extractor on Linux}
\subsubsection{Bulk Extractor on Macintosh}

\sgraphic{ch-1/windows-console}{Setting your Windows console for 132
  rows of text and 9999 lines of scrollback will improve the usability
  of many text-based commands.}

\section{Text Editors}\label{sec:text-editors}

\section{PostScript and PDF Viewers}


\section{Making Hex Dumps}

Notepad++ / Install a hex editor plugin

\subsection{Linux}
On Linux systems you may need to explicitly install Python 3. Do so on
Fedora by typing:

\begin{code}
$ (@ \hl{sudo yum install python3} @) 
\end{code} 
% $

\subsubsection{Hex Dumps under Linux (Fedora)}

Popular tools for creating hex dumps on Linux include \texttt{od}
(octal dump) and \texttt{xxd}.  The \texttt{od} is part of the base
release. xxd must be installed from the package
\texttt{vim-common} with the yum command:

\begin{code}
$ (@ \hl{sudo yum install vim-common} @)
\end{code}

Note: We're not sure why xxd is part of
  vim-common. If you didn't know this, you could could the command
  \texttt{yum whatprovides} to learn which package provides it:
\begin{code}
$ (@ \hl{yum whatprovides xxd} @)
Loaded plugins: langpacks, presto, refresh-packagekit
2:vim-common-7.3.712-1.fc18.x86_64 : The common files needed by any version of the VIM editor
Repo        : fedora
Matched from:
Filename    : /usr/bin/xxd
$
\end{code} 
%$


\section{Hashing}

In computer science a \emph{hash function} is any function that maps a
sequence of zero or more characters (called a \emph{string}) to an
binary number of a specific fixed size---that is, a fixed number of
bits. A 16-bit hash function can produce $2^{16}=65,536$ different hash values, while a
32-bit hash function can produce $2^{32}=4,294,967,296$ possible hash
values. Hash functions are designed so that changing a single
character in the input results in a completely different number. While
the pigeonhole principle guarantees that many different strings will
have the same hash value---something that's called a \emph{hash
  collision}---the more bits in the hash has, the smaller the chance
of a collision.

Hashing was invented by H.\ P.\ Luhn in a 1953 IBM technical memo;
it's been widely used for computerized text processing since the
1960s. For example, because every sentence in a document can be
treated as a string, hashing makes it possible to rapidly see if the
same paragraph ever repeats in a long document: just computer the hash
value for each paragraph, put all of the hashes into a list, sort the
list, and see if any number repeats twice. If there is no repeat, then
no paragraph is repeated. Of course, if a number \emph{does} repeat,
then it's necessary to look at the paragraphs that correspond to those
two numbers to determine if the same paragraph really is repeated
twice or the duplicate is the result of a hash collision. Using hashes
in this manner is much faster than working directly with the
paragraphs because it is much faster for computers to compare numbers
than sequences of words---even when you take into account the time to
perform the hashing.

(Interesting fact: The name \emph{hash} comes from the way hash functions are typically
implemented as a two-step process that first chops and then mixes the
data, much in the way that one might make hash in the kitchen!)

In 1979 Stanford University PhD student Ralph Merkle invented a way to use
hashing for computer security\citep{merkle:79}. Merkle's idea was to
use a hash
function that produced more than 100 bits of output and that
additionally had the property of being \emph{one-way}. That is, it was
a function for which it was relatively easy to compute the hash of a
string, but it was nearly impossible, given a hash, to find a
corresponding string. The essence of Merkle's idea was to use a
document's 100-bit one-way hash as a stand-in for the document itself
for certain mathematical purposes. For example, instead of digitally
certifying a 50-page document, the document could be reduced to a
100-bit hash and the hash could then be certified. Because there are so many different possible hash values
($2^{100}\approx10^{30}$), Merkle reasoned that it would not be
possible for an attacker to take the digital signature from one document and use it
to certify a second document---because to do so would require that
both documents had the same hash value.

Today digital signatures applied to hashes are
the basis of many cyber security system. Digital signatures protect
credit card numbers sent over the Internet, certify the authenticity
and integrity of code run on iPhones, and validate keys used play
digital music. The idea of hashing has been applied to other areas as
well---in particular, forensics.

One of the first and continuing uses of hashing in DF was to establish
\emph{chain-of-custody} for forensic data. Instead of hashing a
document or a file, the hash function is applied to the entire disk
image---that is, \emph{all} of the bytes extracted from a hard
drive. Many law enforcement organizations will create two
disk images of a drive and then computer the hash of each image.  If the values match, then the
copies are assumed to each be a true copy of the data that were on the
drive. The hash value is then included in the official report. With
the hash recorded, the image file can be copied to a server at the
police department, to an investigator's workstation, or even given to
a forensic expert working for the other side. Any investigator with the
data can calculate the hash and see if it matches the original
reported value. If the hashes match, the investigator can be sure that
not a single bit in the disk image has been changed since the original
image was recorded. Hashing is so important that many DF tools can
automatically validate an evidence file by recomputing the hash and
comparing it with a stored value.

A second use for hashing is to identify specific
files. This approach takes advantage of the property that it is
extraordinarily unlikely for two files to have the same
hash value. File hashes can thus be used to identify files in much the
same way that a person can be identified by their fingerprints. 

Today forensic practitioners distribute databases containing the file
hashes as a standard part of forensic processing. The best known of these data
sets is the National Software Reference Library Reference Data Set,
distributed by the National Institute of Standards and
Technology\cite{nist-nsrl-rds-march2012}. These data sets can be used
to identify \emph{known goods}, such as programs distributed as part
of operating systems, or \emph{known bads} such as computer viruses,
stolen documents and child of pornography. Recent work is now
applying cryptographic hashing to blocks of data smaller than
files\cite{garfinkel:sector-id}, taking advantage of the fact that
even relatively short 512-byte and 4096-byte segments taken from
files can be highly identifying. 

File and sector identification with hashing means that a hard drive
containing millions of files can be automatically searched against a
database containing the hashes of hundreds of millions of file hashes
in a relatively short amount of time---perhaps just a few hours. Importantly, the search can be done without any human
intervention.



