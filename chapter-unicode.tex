\chapter{Unicode}
\label{chap:unicode}

Unicode is a continual source of frustration and problems for
developers and users of digital forensic tools. Designed in the 1990s
as a single code set capable of representing all of the world's
languages in the past, present and future, Unicode has largely
realized this goal. Today you can type languages with non-Roman
characters such as Russian or Japanese into a web browser or word
processor and generally produce acceptable looking documents. Most
application programs will even work with right-to-left languages like Arabic, 
Hebrew and Syriac, although support is somewhat inconsistent. Unicode
has also been a boon for graphic designers, who now have a dizzying
array of symbols with which they can work.

Unicode has been less kind to the world of digital forensics, for the
simple reason that Unicode makes it possible to have a wide variety of
byte representations within digital documents that display the same
way on a computer screen, print the same way on page, and are required
by the Unicode standard to be interpreted in the same manner.  As of
this document's writing in 2011, the majority of computer forensic
programs are simply not up to the challenge. 

As a result we see frequently characteristic failings when forensic
tools are presented with Unicode text:
\begin{itemize}
\item Unicode text frequently does not display properly, and it may
  not display at all.
\item Searches performed with forensic tools may not find names and
  keywords that are clearly 
  visible when a document is viewed with a native application.
\item \emph{Any other problems?}
\end{itemize}

One reason for the poor coverage of computer forensic tools may be
that most of today's tools are developed in the United States, and it
is possible to work broadly with computer systems in the US using
ASCII alone. Another issue may be that many computer forensic tools
are written in the C programming language, and the standard C string
type (\emph{char *}) cannot be used to represent Unicode that is in
the UTF-16 or UTF-32 encodings, as strings in these encodings invariably
contain NULL characters. Software developed outside the US tends to
have much better support for Unicode---perhaps for the simple reason
that currency symbols used by most countries of the world (\eg \pounds, \EURtm\xspace and \textyen)
cannot be represented using 7-bit codes, and thus programmers outside
the US are frequently forced to deal with these issues.

\begin{figure}
\begin{description}
\item[ASCII]
\item[code point]
\item[Baudot]
\item[EBCDIC]
\item[encoding]
\item[glpyh]
\item[grapheme]
\item[font]
\item[Shift JIS]
\item[Unicode]
\item[UTF-7]
\item[UTF-8]
\item[UTF-16]
\item[UTF-16BE]
\item[UTF-16LE]
\end{description}
\caption{Terminology}
\end{figure}

\section{Codes and Semantic Meaning}

The mechanization of writing has had a fundamental limitation from the
days of Gutenberg to the modern age. An unlimited number of
characters and designs can be created with pen and paper, but until
recently mechanical systems were limited to a relatively small number
of \emph{glpys} that could be represented. For example, in order to
print with movable type (as Gutenberg did), it is necessary to have a
piece of piece of \emph{type} for each glyph that is intended for the
printed page. 

\sgraphic{art/Metal_movable_type.jpg}{Movable metal type, and
  composing stick, descended from Gutenberg's press. Note: the plate says - ``The
  quick brown fox jumps over the lazy dog and feels as if he were in
  the seventh heaven of typography together with Hermann Zapf, the
  most famous artist of the''.  Photo by Willi Heidelbach; used with
  permission. See \texttt{http://en.wikipedia.org/wiki/File:Metal\_movable\_type.jpg}}

\emph{TK: Insert more information about the history of alphabetical
  order and the assignment of numbers to letters.}

\emph{TK: Insert information about Morse Code}

\subsection{Baudot and 5-Level Code for Teleprinters}

\emph{TK: Insert something about Baudot}

\subsection{ASCII}

\emph{TK: History of ASCII}

\subsection{Attempts to Internationalize ASCII}

\emph{TK: Brief history of font representation in Asia and why it
  matters for forensics.}

\subsection{Unicode}

\emph{TK: Brief history of Unicode through Unicode 5}

Unicode 1.0 (1991--1995) was a 16-bit code. Microsoft Windows still uses this version of Unicode for file names, and as a result certain Unicode characters cannot be used in filenames.

Starting with Unicode 2.0 (July, 1996) Unicode was expanded to cover the range U+0000..U+10FFFF. Each Unicode code point can be \emph{encoded} as a sequence of bytes. Somewhat confusingly, there are many different sequences of bytes that can be used to represent the same code point. Each of these encoding systems has advantages for different applications, so we have them all and Unicode-aware string libraries can easily convert from one encoding to another.

The most common Unicode encoding in the Western world is UTF-8, a
variable length code that  codes ASCII  as  byte values and other
scripts with sequences between two and four bytes.  UTF-16 codes code
points U+0000..U+FFFF as 16-bit values and other values with four bytes. UTF-32 codes all Unicode characters as four bytes, which is the least efficient space wise but which is easiest to work with inside a computer system. UTF-16 and UTF-32 both come in Big Endian and Little Endian varieties. (UTF standards for \emph{Unicode transforamtion format}.)

Unicode has 66 invalid code points, including U+FFFE and U+FFFF.

Remember that Unicode is used to encode \emph{scripts}, not languages. Scripts are used for displaying languages in written form. This is an important distinction, because the LATIN CAPITAL LETTER A (``A'', or U+0041) () is a A no matter whether the language is English or French. On the other hand, the GREEK CAPITAL LETTER ALPHA (``\includegraphics{uni/unicode_alpha}'', or U+0391) looks like an A, but it isn't. This is done so that sort order will be preserved within alphabets, and to preserve round-tripping between Unicode and other coding formats.

\section{Unicode Today: Design and the Forensic Challenge}

When working with Unicode in a forensic context, the most important
thing to remember is that Unicode is fundamentally based on \emph{code
points} and not on the specific \emph{encoded characters} that we may
encounter when performing forensic analysis.

A Unicode code point is an
idealized character that has a value. The first 127 of these map to
ASCII (the American Standard Code for Information Interchange). You
may be aware that capital A is a 65 (or 0x41). In Unicode most
characters are between 0 and 65535; they are written as a U+ followed
by a 4-character hex code, so the capital A is U+0041. Each Unicode
code point has a name as well, which is traditionally typed using
CAPITAL LETTERS.

The Unicode Character Database defines all of the Unicode code points. Different ranges of code points are used for different purposes. The first 10 ranges are shown in \tabref{unicode-ranges}
\begin{table}
\begin{Verbatim}
0000..007F; Basic Latin
0080..00FF; Latin-1 Supplement
0100..017F; Latin Extended-A
0180..024F; Latin Extended-B
0250..02AF; IPA Extensions
02B0..02FF; Spacing Modifier Letters
0300..036F; Combining Diacritical Marks
0370..03FF; Greek and Coptic
0400..04FF; Cyrillic
0500..052F; Cyrillic Supplement
0530..058F; Armenian
0590..05FF; Hebrew
0600..06FF; Arabic
0700..074F; Syriac
0750..077F; Arabic Supplement
0780..07BF; Thaana
07C0..07FF; NKo
\end{Verbatim}
\caption{Unicode code point blocks for Unicode code points between \texttt{U+0000} and \texttt{U+07FF}. The entire file can be found at \texttt{http://unicode.org/Public/UNIDATA/Blocks.txt}}\label{unicode-ranges}
\end{table}

Note that while Unicode code points are frequently referred to as Unicode
``characters,'' this isn't strictly true, as code points can represent
information other than printable glyphs. Unicode has all of ASCII's
non-printing ``characters,'' including SPACE (U+0020) and LINE
FEED (U+000A) and BELL (U+0007). Unicode also has a spaces associated
with typesetting, including EM SPACE (U+2003), UN SPACE (U+2002), and
even THIN SPACE (U+2009). But Unicode also has special codes such as 
LEFT-TO-RIGHT MARK (U+200E) and RIGHT-TO-LEFT MARK (U+200F) that
control the direction that the glyphs are laid on the page. 

Unicode code points can be encoded in multiple variants. UTF-8 is the
most common in the US. UTF-8 encodes all ASCII characters as
themselves, and codes the rest as a variable length code between 2 and
6 characters long. This doesn't work so well for Chinese, most of
which take 3 bytes in UTF-8.  So in China and other Asian countries,
you'll find a lot of UTF-16, which codes most Unicode characters as
2-bytes. If the big byte comes first it is UTF-16BE (``Big Endian''),
with the capital A encoded as 0x00 0x41. If the little byte comes
first the coding is UTF-16LE (``Little Endian''), with the capital A
encoded as 0x41 0x00. There is also an encoding called UTF-7, in which
every character is coded within the ASCII character set. Avoid it.

Ideally every block of  Unicode-encoded data is preceeded with the Byte Order
Mark, the Unicode ZERO WIDTH NO-BREAK
SPACE (U+FEFF). This code point, stored at the beginning Unicode
block, allows the reader to infer if the data is stored as UTF-8, UTF-16LE, UTF-16BE, UTF-32LE or
UTF-32BE. Byte order detection relies on the fact that U+FFFE is an
invalid Unicode character. If the beginning of  file contains the
hex characters |FE FF| then the file is UTF-16BE because the big
character is first. \tabref{table-bom} shows various codings of the
Unicode byte order mark.

Unfortunately, many (perhaps most) sequences of Unicode-encoded bytes
do not being with the BOM. Instead, coding is provided by a higher
level protocol. For example, many HTML pages begin with the following
four lines to indicate that the web page is stored as UTF-8:

\begin{Verbatim}
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-US" lang="en-US">
<head>
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
\end{Verbatim}

The observant reader will note that decoding the HTML page to
determine the page's encoding requires knowing the page's
encoding, which is not possible. In practice web browsers attempt to
infer the pages encoding by trying multiple decodings of the first few
bytes of the file until they discover the |<!DOCTYPE| or |<html| tag.


\begin{table}
\begin{tabular}{cl}
Unicode Encoding & Hex character sequence\\
\hline
UTF-8 & |EF BB BF|\\
UTF-16BE & |FE FF|\\
UTF-16LE & |FF FE|\\
UTF-32BE & |00 00 FE FF|\\
UTF-32LE & |FF FE 00 00|\\
\end{tabular}
\caption{Encodings and representations of Unicode ZERO WIDTH NO-BREAK
SPACE (U+FEFF), the Unicode byte order Mark.}\label{table-bom}
\end{table}


\subsection{Unicode Goals}

\emph{TK: Insert Unicode Goals}

\subsection{Unicode 6.0}

\section{Unicode and Forensic Software}

Unicode poses a variety of challenges for authors of forensic
software:

\begin{enumerate}
\item Although many Unicode strings can be freely inter-converted to
  ASCII, the vast majority cannot. This means that code written and
  tested in the United States frequently fails when presented with
  real-world Unicode strings from other countries.
\item File names may contain Unicode characters, and those Unicode
  characters are typically stored differently on Windows, MacOS and
  Unix. 
\item For programs running natively on an operating system, different
  APIs may return the same file names with different Unicode
  encodings.
\item Original byte sequences from subject media needs to be
  preserved inside a forensic program. However data structures 
  that are \emph{required} to contain valid Unicode nevertheless may
  actually contain invalid encodings. This means that forensic
  software must be able to handle invalid Unicode data in a sensible manner.

\end{enumerate}

In practice different programmers have resolved the Unicode problem in
different ways. The result is that different forensic tools perform
differently when faced with the same evidence.

\subsection{Unicode Normalization}

Consider the case of the \~n, a letter that is commonly used (and
misused) in Spanish. There are two different ways to represent this in
Unicode: with the Unicode character |U+00F1| (LATIN SMALL LETTER N
WITH TIDLE), and with two character sequence U+006E (LATIN SMALL
LETTER N) followed by |U+0303| (COMBINING TILDE). Not only do these
characters display the same way (as a \~n), \emph{they both describe
  the same  characters}.

The Unicode standard defines two kinds of sameness: \emph{canonical
equivalence} and \emph{compatibility equivalence}.

\begin{description}
\item[Canonical equivalence] results from
the application of one or more combining characters and is considered
a strong equivalence: the abstract character is the same no matter
whether it is represented by a single Unicode code point or by
multiple code points with the aid of combining characters. Likewise
the order in which combining characters are used does not
matter. Combined characters always display the same way as base
characters with combiners. the use of combining characters results in
the same abstract character as the uncombined form. 

% ⑨
% ❾
% 
\PreloadUnicodePage{36}
\item[Compatibility equivalence] is used to denote sequences that have the same semantic
meaning but may appear visually distinct. For example, TR15 says that a
SUPERSCRIPT NINE (⁹) (U+2079) and a SUBSCRIPT NINE (₉) (U+2089) are
compatibility equivalent characters, because they are both a
nine. This is confusing for people who are used to doing superscripts
and subscripts in Microsoft Word or LaTeX; both of those systems do
superscripts and subscripts by shrinking the font and changing the
baseline. The unicode also has CIRCLED DIGIT NINE (U+2468) \includegraphics{uni/unicode_2468} and
DINGBAT NEGATIVE CIRCLED DIGIT NINE \includegraphics{uni/unicode_277E} (U+277E). This means that the
number \includegraphics{uni/unicode_2468}\includegraphics{uni/unicode_2468} and 99 have the same sort order, even though they have
wildly different Unicode characters
 
Another example is HEBREW LETTER ALEF (\includegraphics{uni/unicode_05D0}) (U+05D0) and ALEF SYMBOL (\includegraphics{uni/unicode_2135}))
(U+2135). Wow, those two look really similar. But if you
copy-and-paste the first one into a document you'll see that it's in
right-to-left mode while the second one is in left-to-right
mode. That's because the first one is for hebrew and the second is for
math. There is also HEBREW LETTER WIDE ALEF \includegraphics{uni/unicode_FB21} (U+FB21) which is
probably for use with Japanese (they like having double-wide
characters because it matches the Kanji, and you'll note that this
Unicode's value is up in the FBxx range.


\end{description}

To compare Unicode strings it is necessary to put them into a standard
form, a process called \emph{noramlization} (\figref{unicode-normalization}). In normalization,
sequences of Unicode code points that are part of the same equivalence classes are transformed
so that they have the identical code points. The strings can then be
compared (either directly, code-point-by-code-point, or by
transforming the strings yet again into a Unicode encoding such as
UTF-8 or UTF-32LE). 

\begin{figure}[t]

The Unicode Standard actually defines two kinds of equivalences
between characters: \emph{canonical equivalence} and
\emph{compatibility equivalence.} Canonical equivalence results from
the application of one or more combining characters and is considered
a strong equivalence: the abstract character is the same no matter
whether it is represented by a single Unicode code point or by
multiple code points with the aid of combining characters. Likewise
the order in which combining characters are used does not
matter. Combined characters always display the same way as base
characters with combiners. the use of combining characters results in
the same abstract character as the uncombined form. Compatibility
Equivalence is used to denote sequences that have the same semantic
meaning but may appear visually distinct.) \\
~\\
The Unicode Normalization process is described in  Unicode Technical Annex \#15: Unicode Normalization \url{http://unicode.org/reports/tr15/}. There are  four different normalization forms:\\
\\
\begin{centering}
\begin{tabular}{|l|l|}
\hline
Title & Description\\
\hline
Normalization Form D (NFD)	& Canonical Decomposition\\
\hline
Normalization Form C (NFC)	& Canonical Decomposition,\\
                            & followed by Canonical Composition\\
\hline
Normalization Form KD (NFKD) & Compatibility Decomposition\\
\hline
Normalization Form KC (NFKC) & Compatibility Decomposition,\\
                             &followed by Canonical Composition\\
\hline
\end{tabular}
\end{centering}
\\
The Unicode standard uses the word \emph{singleton} to refer to a character that has a decomposition of a single unicode character.
\\
The NFD form decomposes a combined character into the base character and the combiner, whereas the NFC form does the reverse. As a result, NFD forms result in larger byte representations than NFC forms. The NFKD and NFKC forms add a compatibility decomposition---for example, decomposing the ``fi'' ligature into the characters ``f'' and ``i'' and transforming superscript and subscript numbers to unscripted letters. Among other things, this transformation improves the chances of string searches resolving correctly.
\\
Please refer to UAX \#15 for full details.
\caption{Unicode equivalence and normalization Forms, from UAX\#15}\label{unicode-normalization}
\end{figure}

Unicode  has four different normalized forms. Once normalized, the strings can be directly compared in the Unicode domain (if you have a Unicode string object that can do so).
Alternatively, the strings can then be encoded into a byte-level representation such as UTF-8 and compared byte-for-byte. 

\tabref{unicode-n} shows the legacy approaches for representing the character and the many byte sequences that can be used to represent this character in Unicode. 

\begin{table}
\begin{tabular}{llll}
\hline
Encoding & Hex    & Notes & \\
\hline
Microsoft Windows &  |F1|      &  Type with ALT 241\\
Mac Roman         &  |96|      & Option-n n \\
Linux             &            & Ctrl+Shift u00F1\\
\hline\\
\multicolumn{4}{l}{\emph{Unicode ``n'' with the ``\~{}'' Combining Character:}}\\
UTF-8     & |6E CC 83|\\
UTF-16 BE & |00 6E 03 03|\\
UTF-16 LE & |00 6E 03 03|\\
UTF-32 BE & |00 00 00 6E 00 00 03 03|\\
UTF-32 LE & |6E 00 00 00 03 03 00 00|\\
\hline
\\
\multicolumn{4}{l}{\emph{Normalized Unicode ``\~n'':}}\\
UTF-8   &  |C3 B1|          & \\
UTF-16 BE         &  |00 F1|          & \\
UTF-16 LE         &  |F1 00|          & \\
UTF-32 BE         &  |00 00 00 F1|    & \\
UTF-32 LE         &  |F1 00 00 00|    & \\
\hline
\end{tabular}
\caption{A variety of ways to represent the character "\~n" (LATIN SMALL LETTER N WITH TILDE) in Unicode, and a small Python program that will create a file containing the character.}\label{unicode-n}
\end{table}

\subsection{Unicode and Python}

The Python programming language  support for Unicode and most program written in Python will more-or-less work with Unicode data without significant work on the part of the programmer. This does not mean that you can ignore Unicode issues, however. If you write programs that work with user-supplied data (as all forensic programs do), then you must be sure that your program operates properly with Unicode no matter what format it is provided. Python makes this somewhat easier with built-in functions for transforming Unicode strings into encoded representation and the |unicodedata| class that has a large database of Unicode information.

Unfortunately, support for Unicode changed significantly between Python 2 and Python 3. Because both versions of Python are widely in use, this section will provide a small introduction to each.

\subsubsection{Unicode and Python 2}
Python 2 has support for two kinds of strings. The |string| data type is for traditional strings of single-byte characters (ASCII or Windows-1252), for which the number of characters in the string is precisely equal to the number of bytes. Python 2 also has a |unicode| data type which is an array of |unichar| characters. Unicode strings are specified by placing a lower-case |u| before the quotation mark that starts the string.

Python 2 assumes that your program will be written in ASCII, but you can change this default by specifying a coding on the first or second line of your program. If you open a file with |open()| and read the bytes the result will be type |str|.

Below is a small program that was used to create the data in \tabref{unicode-n}:

\begin{Verbatim}
# -*- coding: utf-8 -*-
#

import unicodedata,sys

encodings=['mac roman','latin1','utf-8','utf-16 le','utf-16 be',
           'utf-32 le','utf-32 be']
nn     = unicodedata.normalize("NFC",u"ñ")
print type(nn),unicodedata.name(nn)
for encoding in encodings:
  print "%9s: " % format(encoding),
  for ch in nn.encode(encoding):
      print "%02x " % ord(ch),
  print
\end{Verbatim}

Notice that there's a small slight-of-hand going on with this
program. In the actual program code the ``ñ'' is stored as the hex
sequence |C3 B1|. However when Python reads this sequence the parser
automatically decodes the UTF-8 into the Unicode code point U+00F1,
which is then put into the string. 

Alternatively, the variable \texttt{nn} could be set with an escaped
Unicode sequence. Normalization is not required because the value is
provided in a normalized form:

\begin{Verbatim}
nn   = u"\u00F1"
\end{Verbatim}

If the character was entered in the source code as a two-byte vector,
it would be necessary to join it into a string and then to decode that
string as UTF-16 before normalization:

\begin{Verbatim}
buf  = ['\x00','\xF1']
nn   = unicode.normalize("NFC",("".join(buf)).decode('utf-16 be'))
\end{Verbatim}

\emph{NOTE: THIS WOULD MAKE MORE SENSE IF WE START THE EXAMPLE BY
  PRESENTING WITH THE ESCAPED UNICODE SEQUENCE AND THEN SHOW HOW TO
  EMBED THE CHARACTER IN THE DOCUMENT.}


And here is the output:

\begin{Verbatim}
$ ./unicode_demo2.py 
<type 'unicode'> LATIN SMALL LETTER N WITH TILDE
mac roman:  96 
   latin1:  f1 
    utf-8:  c3  b1 
utf-16 le:  f1  00 
utf-16 be:  00  f1 
utf-32 le:  f1  00  00  00 
utf-32 be:  00  00  00  f1 
$ 
\end{Verbatim}

If you don't want to put hard-coded UTF-8 characters in your source code file (some editors may complain), the string |u"ñ"| can also be written |u"\u00f1"|. Characters outside the BMP are encoded with 8-digits and a capital U, so the U+1D11E (MUSICAL SYMBOL G CLEF), is entered as |u"\U0001D11E"|.

One last thing---most version of Python use UTF-16 internally, meaning that the Supplementary Planes are accessed with Surrogate Pairs. This can cause some weirdness, because a single code point is actually represented by two UTF-16 characters:

\font\musixfont="musix11" at 7pt
\newcommand\cleff{{\musixfont G}} % G-Cleff symbol from MusixTeX fonts
\begin{Verbatim}[commandchars=\$\{\}]
>>> cleff=u"\U0001D11E"
>>> len(cleff)
2
>>> type(cleff)
<type 'unicode'>
>>> cleff[0]
u'\ud834'
>>> cleff[1]
u'\udd1e'
>>> import sys
>>> sys.stdout.write(cleff.encode('utf-8')+"\n")
$cleff
>>> 
\end{Verbatim}
%$
This problem of surrogates is present in both Python 2 and Python 3
(as well as in the native Windows API). Python can be compiled to use UCS-4
internally, but that isn't done. Allegedly the reason is performance
reasons---UCS-4 strings take up twice the size of UTF-16 strings---but
the total overhead required by 4-byte characters may be minimal. 


\subsubsection{Unicode in Python 3}
In Python 3 all strings are  Unicode strings, and the Unicode type has been abolished.  If you open and read the contents of a file the resulting data type will depend on how the mode and codec provided when the file is opened.

For example, consider a simple file on the Unix system that contains the ASCII string "Hello World!" and a newline:

\begin{Verbatim}
$ echo 'Hello World!' > hello.txt
$ cat hello.txt
Hello World!
$ ls -l hello.txt
-rw-r--r--  1 simsong  staff  13 Oct 23 16:09 hello.txt
$ 
\end{Verbatim}

Opening and reading the content of this file in text mode will yield a Unicode string:
\begin{Verbatim}
>>> open("hello.txt","r").read()
'Hello World!\n'
>>> type(open("hello.txt","r").read())
<class 'str'>
>>> 
\end{Verbatim}

If you want a binary string, you must open the file in binary mode:

\begin{Verbatim}
>>> open("hello.txt","rb").read()
b'Hello World!\n'
>>> type(open("hello.txt","rb").read())
<class 'bytes'>
>>> 
\end{Verbatim}

Notice that there is a |b| at the beginning of the string, indicating that it is a byte string, and not a Unicode string.

The distinction between bytes and Unicode text is important when you are trying to read data from files that is not valid Unicode. Consider a file that is filled with the 16 hex FF characters:

\begin{Verbatim}
$ ls -l file.txt 
-rw-r--r--  1 simsong  staff  16 Oct 23 16:15 file.txt
$ xxd file.txt 
0000000: ffff ffff ffff ffff ffff ffff ffff ffff  ................
$ 
\end{Verbatim}
%$
The issue here is that the sequence |FF FF FF FF| is not valid Unicode
in \emph{any} Unicode encoding (there is no U+FFFF). Python 3 defaults
to text mode, which means it expects all text files that are read to contain valid Unicode. Therefore, if we try to read this file with a legacy Python 2 program using Python 3, we'll get a puzzling error:

\begin{Verbatim}
>>> f = open("file.txt","r")
>>> f.read()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/local/Library/Frameworks/Python.framework/Versions/3.2/lib/python3.2/codecs.py", line 300, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf8' codec can't decode byte 0xff in position 0: invalid start byte
>>> 
\end{Verbatim}

The solution is to open the file in binary mode:

\begin{Verbatim}
>>> f = open("file.txt","rb")
>>> f.read()
b'\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff\xff'
>>> 
\end{Verbatim}

And here is a rewrite of the Unicode-aware program we showed in the previous section:
\begin{Verbatim}
#!/usr/bin/env python3.2 
# -*- coding: utf-8 -*- 
# 

import unicodedata
encodings=['latin1','utf-8','utf-16 le','utf-16 be',
           'utf-32 le','utf-32 be']
nn     = unicodedata.normalize("NFC","ñ")
print(type(nn),unicodedata.name(nn))
for encoding in encodings:
  print("{:9}: ".format(encoding),end="")
  for ch in nn.encode(encoding):
      print("{:02x} ".format(ch),end="")
  print()
\end{Verbatim}

Python 3 also lets you write the string string |"ñ"| as|"\u00f1"|. Note that you do not need to prefix the string with a |u| (in fact, you can't) because all Python 3 strings are unicode.


\subsubsection{The Python \texttt{unicodedata} module}
You should become familiar with the Python |unicodedata| module. This module, present in both Python 2 and 3, provides a number of functions including:

\begin{itemize}
\item |lookup()| and |name()|, to look up a Unicode character by name and return the name of a unicode character.
\item |category()|, to return the category to which a Unicode character is defined.
\item |normalize()|, an implementation of the Unicode normalization algorithm.
\end{itemize}

\subsubsection{Missing from Python's Unicode Support}
Christopher Lenz notes that Python's Unicode implementation is actually quite poor and contains only the minimum needed.\url{http://www.cmlenz.net/archives/2008/07/the-truth-about-unicode-in-python} In particular, he notes, there is no support in Python 2 or 3 for:

\begin{itemize}
\item Collation
\item Special case conversion
\item Regular expressions
\item Text segmentation
\item bidirectional text handling
\end{itemize}

What this means in practice is that Python 3 thinks that the string |'café'| sorts lexicographically after |'caff'| because |é| (U+00E9, LATIN SMALL LETTER E WITH ACUTE) has a higher character code than |f| (U+0066, LATIN SMALL LETTER F). 

He's right, but it's not clear that this is going to be fixed anytime soon.

\subsection{Unicode and C/C++}
There are two primary ways of representing Unicode strings in C/C++ programs:
\begin{enumerate}
\item The string can be encoded in UTF-8 and stored in a |char *| buffer or a or C++ |std::string|. This approach takes advantage of the fact that UTF-8 encoding never codes NULLs.
\item The string can be stored ``natively'' in with ``wide characters,'' the C/C++ |wchar_t| type and the C++ |std::wstring|.   Windows XP uses a short to hold a |wchar_t|, while Linux and MacOS use a 4-byte value and store UTF-32 characters.  
\end{enumerate}

Windows handles Unicode characters outside the BMP through the use of Surrogate characters. Surrogates are essentially combinations of two UTF-16 characters to stand for a single Unicode character that has a value over U+FFFF. Windows Vista has three macros for working with surrogate pairs, |IS_HIGH_SURROGATE|, |IS_LOW_SURROGATE|, and |IS_SURROGATE_PAIR|. Support for surrogate pairs is spotty throughout the Windows ecology. More information can be found at \url{http://msdn.microsoft.com/en-us/library/windows/desktop/dd374069%28v=vs.85%29.aspx}.

Conversions between various encodings can be done with GNU libiconv (\url{http://www.gnu.org/s/libiconv/}). If you are using C++ you may wish to use the UTF8-CPP module (\url{http://utfcpp.sourceforge.net/}).





\subsection{Unicode and Java}
Java officially uses Unicode as its character and string encoding. Unfortunately the Java standard did not adopt Unicode code points as an abstract representation---it adopted a 16-bit representation for characters. That's because Java was standardized in the day of Unicode 1.0, when Unicode was a 16-bit code.

\emph{How does Java handle things outside of the BMP?}


\subsection{\be and Unicode: a case history}
One tool that has been modified to understand Unicode is
\be, the author's bulk data analysis tool. Making
this tool Unicode-aware has been a time consuming and error-prone
task, and it is not finished yet. One of the primary functions of \be
is to find and report all URLs that appear in a disk image under
analysis. Unicode impacts this task in at least four ways:

\begin{enumerate}
\item URLs may be encoded as ASCII, UTF-8 or UTF-16 in the disk image
  under analysis. Thus it is necessary for \be to recognize a URL in
  any of these variant encodings.\label{url-encoding}
\item \be produces a histogram that reports the number of times each
  URL appears in the disk image. Thus, it is necessary to recognize
  that URLs present in different encodings represent the same URL.\label{url-matching}
\item The URL's \emph{path} may contain unicode characters.\label{url-path}
\item The URL's \emph{domain name} may contain an internationalized
  host name.\label{url-domain}
\end{enumerate}

Currently \be handles the first two of these problems.

\subsubsection{Evolution of Unicode Handling in \be}
When it was first released in 2010, \be had a simplistic handling of
Unicode URLs: the program simply looked for text strings in which
every-other character was an ASCII NULL and removed the
NULLs. Non-ASCII characters in output were changed to the underbar
character. This approach handled case \ref{url-encoding} and case
\ref{url-matching} above, but it caused a problem for users: the data
reported by \be did not actually exist. (See
\figref{url_histogram-0.7.16} for an example of the output from \be's
  \emph{url\_histogram.txt} and \emph{url.txt} files.)

As the result of user complaints, Version 1.0 of \be was modified to
report the actual text extracted from the disk image, even for cases
in which the text was UTF-16. Now the information in the \be report
was an accurate representation of what was in the disk
image. Although this output worked well with automated processing, it
made  manual review of the file considerably harder (\figref{url_histogram-1.0.2}).

For version 1.1 of \be the histogram generation routine was modified
to detect the presence of UTF-16LE and UTF-16BE in the feature
files. These strings are transformed to UTF-8 before the histogram
bins are calculated. The revised histogram program also tracks how
many of the source strings were transformed and reports this in the
histogram output as well (\figref{url_histogram-1.1.0}). The result is
tool output that is correct, usable for automated processing, and
usable for manual analysis.


\begin{figure}
\begin{Verbatim}[fontsize=\footnotesize]
n=8164  http://components.groove.net/Groove/Components/Root.osd?Package=net.groove.Groove.Tool
  Components.GrooveCommonComponents_DLL&Version=0
n=6257  http://www.microsoft.com/contentredirect.asp.
n=3031  http://ocsp.verisign.com0

11548143368     http://components.groove.net/Groove/Components/Root.osd?Package=net.groove.Gro
  ove.ToolComponents.GrooveCommonComponents_DLL&Version=0       ___URL(_http://components.groo
  ve.net/Groove/Components/Root.osd?Package=net.groove.Groove.ToolComponents.GrooveCommonCompo
  nents_DLL&Version=0&Factory
11548144224     http://components.groove.net/Groove/Components/Root.osd?Package=net.groove.Gro
  ove.ToolComponents.GrooveCommonComponents_DLL&Version=0       ___URL(_http://components.groo
  ve.net/Groove/Components/Root.osd?Package=net.groove.Groove.ToolComponents.GrooveCommonCompo
  nents_DLL&Version=0&Factory
11548145356      http://components.groove.net/Groove/Components/Root.osd?Package=net.groove.Gr
  oove.ToolComponents.GrooveCommonComponents_DLL&Version=0      ___URL6_http://components.groo
  ve.net/Groove/Components/Root.osd?Package=net.groove.Groove.ToolComponents.GrooveCommonCompo
  nents_DLL&Version=0&Factory
\end{Verbatim}
\caption{The first three lines of the \texttt{url\_histogram.txt}
  output file from \be version 0.7.16 processing the
  nps-2009-domexusers disk image, followed by three lines taken from
  the \texttt{url.txt} file of where the groove.net URL was
  found. Long lines have been wrapped; the continuation line begins
  with two spaces}\label{url_histogram-0.7.16}
\end{figure}

\begin{figure}
\begin{Verbatim}[fontsize=\footnotesize]
n=8164  h\000t\000t\000p\000:\000/\000/\000c\000o\000m\000p\000o\000n\000e\000n\000t\000s\000.
  \000g\000r\000o\000o\000v\000e\000.\000n\000e\000t\000/\000G\000r\000o\000o\000v\000e\000/\0
  00C\000o\000m\000p\000o\000n\000e\000n\000t\000s\000/\000R\000o\000o\000t\000.\000o\000s\000
  d\000?\000P\000a\000c\000k\000a\000g\000e\000=\000n\000e\000t\000.\000g\000r\000o\000o\000v\
  000e\000.\000G\000r\000o\000o\000v\000e\000.\000T\000o\000o\000l\000C\000o\000m\000p\000o\00
  0n\000e\000n\000t\000s\000.\000G\000r\000o\000o\000v\000e\000C\000o\000m\000m\000o\000n\000C
  \000o\000m\000p\000o\000n\000e\000n\000t\000s\000_\000D\000L\000L\000&\000V\000e\000r\000s\0
  00i\000o\000n\000=\0000\000
n=6257  h\000t\000t\000p\000:\000/\000/\000w\000w\000w\000.\000m\000i\000c\000r\000o\000s\000o
  \000f\000t\000.\000c\000o\000m\000/\000c\000o\000n\000t\000e\000n\000t\000r\000e\000d\000i\0
  00r\000e\000c\000t\000.\000a\000s\000p\000.\000
n=3031  http://ocsp.verisign.com0


11548143368     h\000t\000t\000p\000:\000/\000/\000c\000o\000m\000p\000o\000n\000e\000n\000t\0
  00s\000.\000g\000r\000o\000o\000v\000e\000.\000n\000e\000t\000/\000G\000r\000o\000o\000v\000
  e\000/\000C\000o\000m\000p\000o\000n\000e\000n\000t\000s\000/\000R\000o\000o\000t\000.\000o\
  000s\000d\000?\000P\000a\000c\000k\000a\000g\000e\000=\000n\000e\000t\000.\000g\000r\000o\00
  0o\000v\000e\000.\000G\000r\000o\000o\000v\000e\000.\000T\000o\000o\000l\000C\000o\000m\000p
  \000o\000n\000e\000n\000t\000s\000.\000G\000r\000o\000o\000v\000e\000C\000o\000m\000m\000o\0
  00n\000C\000o\000m\000p\000o\000n\000e\000n\000t\000s\000_\000D\000L\000L\000&\000V\000e\000
  r\000s\000i\000o\000n\000=\0000\000       \000\001\000\000\000\003\000\000\200URL(\001\000\0
  00h\000t\000t\000p\000:\000/\000/\000c\000o\000m\000p\000o\000n\000e\000n\000t\000s\000.\000
  g\000r\000o\000o\000v\000e\000.\000n\000e\000t\000/\000G\000r\000o\000o\000v\000e\000/\000C\
  000o\000m\000p\000o\000n\000e\000n\000t\000s\000/\000R\000o\000o\000t\000.\000o\000s\000d\00
  0?\000P\000a\000c\000k\000a\000g\000e\000=\000n\000e\000t\000.\000g\000r\000o\000o\000v\000e
  \000.\000G\000r\000o\000o\000v\000e\000.\000T\000o\000o\000l\000C\000o\000m\000p\000o\000n\0
  00e\000n\000t\000s\000.\000G\000r\000o\000o\000v\000e\000C\000o\000m\000m\000o\000n\000C\000
  o\000m\000p\000o\000n\000e\000n\000t\000s\000_\000D\000L\000L\000&\000V\000e\000r\000s\000i\
  000o\000n\000=\0000\000&\000F\000a\000c\000t\000o\000r\000y\000
\end{Verbatim}
\caption{The first three lines of the \texttt{url\_histogram.txt}
  output file from \be version 1.0.2 processing the
  nps-2009-domexusers disk image, followed by three lines taken from
  the \texttt{url.txt} file of where the groove.net URL was
  found. Long lines have been wrapped and indented with two spaces.}\label{url_histogram-1.0.2}
\end{figure}

\begin{figure}
\begin{Verbatim}
n=8164	http://components.groove.net/Groove/Components/Root.osd?Package=net.groove.Groove.Tool
  Components.GrooveCommonComponents_DLL&Version=0	(utf16=8164)
n=6257	http://www.microsoft.com/contentredirect.asp.	(utf16=6257)
n=3279	http://ocsp.verisign.com
\end{Verbatim}
\caption{The first three lines of the \texttt{url\_histogram.txt}
  output file and the first line of the \texttt{url.txt} files created
  by processing the
  nps-2009-domexusers disk image with \be version 1.1.0. The output from the \texttt{url.txt}
file is not included, as it is the same as in \figref{url_histogram-1.0.2}}\label{url_histogram-1.1.0}
\end{figure}

\section{Forensic Research in Unicode}

One way to understand UNICODE is to read the Unicode standard. Much of
it is available online, including the Code Charts. You can find it at:
http://www.unicode.org/versions/Unicode6.0.0/

``Unicode: A Universal Character Code,'' Digital Technical Journal,
Vol. 5, No. 3, Summer 1993. It's attached to this message.

Markus Kuhn, a computer security expert at Cambridge has put together
a UTF-8 and Unicode FAQ for Unix/Lunux. I highly recommend it, but you
may feel like you are coming into the middle of a discussion.
http://www.cl.cam.ac.uk/~mgk25/unicode.html



\section{Technical References to Review}
\begin{itemize}
\item Unicode Standard Annex \#15 --- Unicode Normalization Forms (\url{http://unicode.org/reports/tr15/})
\item Unicode Technical Report \#17 --- Unicode Character Encoding Model (\url{http://www.unicode.org/reports/tr17/}), discusses terms and encodings used in the Unicode standard.
\item Unicode Technical Report \#25 - Unicode Support for Mathematics \url{http://www.unicode.org/reports/tr25/}
\item Unicode Technical Report \#36 - Unicode Security Considerations \url{http://unicode.org/reports/tr36/}

\end{itemize}



% LocalWords:  encodings
